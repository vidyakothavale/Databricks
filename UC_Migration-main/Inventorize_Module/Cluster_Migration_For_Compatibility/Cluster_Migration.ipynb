{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90d4450-d6dd-4b39-85c9-d5ae5e629c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before running the code, please go through the README file to understand it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3253edab-22f4-4975-a10c-5b57b3356428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note:\n",
    "1.Please run the notebook on a cluster with UC enabled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46dee438-a72d-42f5-ad6c-0eba064e3ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: 2.The UC cluster does not support the No Isolation Shared mode; therefore, we changed the cluster to Single User Access mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21bb38a-6f33-45f6-879d-ddcccd9a6518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Install Pyyaml library\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90bc10c2-be58-4a57-bf9e-d8a187a6d02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import the library\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a5150d-66e4-4ebe-b0e4-f705d26984f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/Workspace/Users/mahesh.pasalapudi@digivatelabs.com/cluster_migration_updated/config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb92e03-abf1-47e3-a8c2-978a2ed200fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extract configuration values from the loaded YAML file\n",
    "DATABRICKS_INSTANCE =config.get(\"DATABRICKS_INSTANCE\")\n",
    "TOKEN = config.get(\"TOKEN\")\n",
    "new_runtime_version = config.get(\"new_runtime_version\")\n",
    "cluster_id = config.get(\"cluster_id\")\n",
    "cluster_ids_to_revert = config.get(\"cluster_ids_to_revert\")\n",
    "my_volume_path = config.get(\"my_volume_path\")\n",
    "cluster_ids = config.get(\"cluster_ids\")\n",
    "single_user_name = config.get(\"single_user_name\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5edaf1ab-3110-463e-9fb9-44bef3f89cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before start the processs store the cluster details for backup, this json files store in same working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb315ec-74cb-4115-8c0d-dae789d7a54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "CLUSTER_LIST_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/list\"\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def save_cluster_details_to_file(cluster_id, cluster_details):\n",
    "    \"\"\"Save the current cluster details to a JSON file.\"\"\"\n",
    "    file_name = f\"cluster_backup_{cluster_id}.json\"# want to store in volume replace the path with volume path\n",
    "    with open(file_name, \"w\") as file:\n",
    "        json.dump(cluster_details, file, indent=4)\n",
    "    print(f\"Cluster details for {cluster_id} saved to {file_name}\")\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Fetch the current cluster details and return them as a dictionary.\"\"\"\n",
    "    response = requests.get(CLUSTER_GET_URL, headers=HEADERS, params={\"cluster_id\": cluster_id})\n",
    "    if response.status_code == 200:\n",
    "        return response.json() \n",
    "    else:\n",
    "        print(f\"Failed to get cluster details: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def get_cluster_list():\n",
    "    \"\"\"Fetch the list of clusters.\"\"\"\n",
    "    response = requests.get(CLUSTER_LIST_URL, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        clusters = response.json().get(\"clusters\", [])\n",
    "        return [cluster['cluster_id'] for cluster in clusters]\n",
    "    else:\n",
    "        print(f\"Failed to get cluster list: {response.text}\")\n",
    "        return []\n",
    "\n",
    "# Fetch the list of cluster IDs\n",
    "cluster_ids = get_cluster_list()\n",
    "\n",
    "# Fetch the details of each cluster and save them\n",
    "for cluster_id in cluster_ids:\n",
    "    cluster_details = get_cluster_details(cluster_id)\n",
    "    if cluster_details:\n",
    "        save_cluster_details_to_file(cluster_id, cluster_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b105929c-1ee6-4617-87ba-a40d7839ebb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cluster Validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e1b6b09-81f6-43e7-b591-1f6f85bf207f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define your Databricks instance URL and token\n",
    "# DATABRICKS_INSTANCE = \"<Your Databricks Instance URL>\"\n",
    "# TOKEN = \"<Your Databricks API Token>\"\n",
    "\n",
    "# Define API URLs\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Fetch the current cluster details and return them as a dictionary.\"\"\"\n",
    "    response = requests.get(CLUSTER_GET_URL, headers=HEADERS, params={\"cluster_id\": cluster_id})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  \n",
    "    else:\n",
    "        print(f\"Failed to get cluster details: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def check_cluster_details(cluster_id):\n",
    "    \"\"\"Check all available cluster details.\"\"\"\n",
    "    cluster_details = get_cluster_details(cluster_id)\n",
    "\n",
    "    if cluster_details:\n",
    "        # Print the entire cluster details for inspection\n",
    "        print(\"Cluster Details:\")\n",
    "        print(json.dumps(cluster_details, indent=4))  \n",
    "        \n",
    "        # Check Unity Catalog (looking for data_security_mode, single user mode indicates Unity Catalog)\n",
    "        unity_catalog_status = cluster_details.get('data_security_mode', 'Not Available')\n",
    "        \n",
    "        if unity_catalog_status == 'SINGLE_USER':\n",
    "            print(\"Unity Catalog is enabled on the cluster.\")\n",
    "        else:\n",
    "            print(f\"Unity Catalog status: {unity_catalog_status}\")\n",
    "        \n",
    "        # Check Photon enabled (check if it's part of the spark version or configuration)\n",
    "        spark_version = cluster_details.get('spark_version', 'Not Available')\n",
    "        if \"photon\" in spark_version.lower():\n",
    "            print(\"Photon is enabled for this cluster.\")\n",
    "        else:\n",
    "            print(\"Photon is not enabled for this cluster.\")\n",
    "        \n",
    "        # Check Runtime Version\n",
    "        runtime_version = cluster_details.get('spark_version', 'Not Available')\n",
    "        print(f\"Runtime Version: {runtime_version}\")\n",
    "\n",
    "        # Check Environment Variables\n",
    "        spark_env_vars = cluster_details.get('spark_env_vars', {})\n",
    "        if spark_env_vars:\n",
    "            print(\"Environment Variables:\")\n",
    "            for var, value in spark_env_vars.items():\n",
    "                print(f\"  {var}: {value}\")\n",
    "        else:\n",
    "            print(\"No environment variables set on the cluster.\")\n",
    "        \n",
    "        # Check Termination Time (if the cluster is terminated)\n",
    "        termination_time = cluster_details.get('terminated_time', 'Cluster still running or not available')\n",
    "        print(f\"Termination Time: {termination_time}\")\n",
    "        \n",
    "        # Check if the cluster is active or terminated\n",
    "        state = cluster_details.get('state', 'Not Available')\n",
    "        print(f\"Cluster State: {state}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Failed to fetch cluster details for validation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # cluster_id = \"0108-050342-xm8nm1ea\"  \n",
    "    # Check all cluster details\n",
    "    check_cluster_details(cluster_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6379598f-55b0-4800-980a-540a645455b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Update Databricks Cluster Runtime and Enable Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32fa48b-d339-4139-b367-e0033de064ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# API Endpoints\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "CLUSTER_EDIT_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/edit\"\n",
    "CLUSTER_RESTART_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/restart\"\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Fetch the current cluster details and return them as a dictionary.\"\"\"\n",
    "    response = requests.get(CLUSTER_GET_URL, headers=HEADERS, params={\"cluster_id\": cluster_id})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  \n",
    "    else:\n",
    "        print(f\"Failed to get cluster details: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def restart_cluster(cluster_id):\n",
    "    \"\"\"Restart the Databricks cluster.\"\"\"\n",
    "    restart_payload = {\n",
    "        \"cluster_id\": cluster_id\n",
    "    }\n",
    "    response = requests.post(CLUSTER_RESTART_URL, headers=HEADERS, json=restart_payload)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Cluster {cluster_id} restart requested successfully.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to restart cluster: {response.text}\")\n",
    "        return False\n",
    "\n",
    "def enable_unity_catalog_on_cluster(cluster_id, new_runtime_version):\n",
    "    \"\"\"Enable Unity Catalog on a Databricks cluster by updating its configuration.\"\"\"\n",
    "    # Fetch current cluster details\n",
    "    cluster_details = get_cluster_details(cluster_id)\n",
    "\n",
    "    if cluster_details:\n",
    "        # Ensure the correct runtime version\n",
    "        cluster_details['spark_version'] = new_runtime_version\n",
    "\n",
    "        # Remove conflicting Spark configurations\n",
    "        spark_conf = cluster_details.get(\"spark_conf\", {})\n",
    "        if \"spark.databricks.passthrough.enabled\" in spark_conf:\n",
    "            print(\"Removing conflicting spark.databricks.passthrough.enabled configuration.\")\n",
    "            del spark_conf[\"spark.databricks.passthrough.enabled\"]\n",
    "\n",
    "        # Prepare the payload to update the cluster configuration\n",
    "        payload = {\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"cluster_name\": cluster_details.get(\"cluster_name\", \"Default Cluster\"),\n",
    "            \"spark_version\": new_runtime_version,  \n",
    "            \"node_type_id\": cluster_details[\"node_type_id\"], \n",
    "            \"driver_node_type_id\": cluster_details[\"driver_node_type_id\"],  \n",
    "            \"num_workers\": cluster_details.get(\"num_workers\"),\n",
    "            \"autoscale\" : cluster_details.get(\"autoscale\"),  \n",
    "            \"autotermination_minutes\": cluster_details.get(\"autotermination_minutes\"), \n",
    "            \"spark_conf\": spark_conf,  \n",
    "            \"aws_attributes\": cluster_details.get(\"aws_attributes\", {}), \n",
    "            \"custom_tags\": cluster_details.get(\"custom_tags\", {}), \n",
    "            \"enable_elastic_disk\": cluster_details.get(\"enable_elastic_disk\", False),\n",
    "            \"init_scripts\": cluster_details.get(\"init_scripts\", []),  \n",
    "            \"single_user_name\": single_user_name, \n",
    "            \"data_security_mode\": \"SINGLE_USER\", \n",
    "            \"runtime_engine\": cluster_details.get(\"runtime_engine\", \"STANDARD\"),  \n",
    "        }\n",
    "\n",
    "        # Send the update request to Databricks API to update the cluster\n",
    "        response = requests.post(CLUSTER_EDIT_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Cluster updated successfully with runtime version: {new_runtime_version}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to update cluster: {response.text}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Unable to fetch cluster details for update.\")\n",
    "        return False\n",
    "\n",
    "def update_cluster_for_unity_catalog(cluster_id, new_runtime_version):\n",
    "    \"\"\"Main function to enable Unity Catalog on a Databricks cluster.\"\"\"\n",
    "    # Enable Unity Catalog on the cluster\n",
    "    if enable_unity_catalog_on_cluster(cluster_id, new_runtime_version):\n",
    "        # If the update was successful, wait for the change to take effect and restart the cluster\n",
    "        print(\"Cluster successfully updated for Unity Catalog. Restarting...\")\n",
    "        # Allow time for the cluster to apply updates (could be customized depending on load)\n",
    "        time.sleep(10)  \n",
    "        \n",
    "        if restart_cluster(cluster_id):\n",
    "            print(\"Cluster successfully restarted.\")\n",
    "        else:\n",
    "            print(\"Failed to restart the cluster.\")\n",
    "    else:\n",
    "        print(\"Failed to enable Unity Catalog on the cluster.\")\n",
    "\n",
    "# Example Execution\n",
    "if __name__ == \"__main__\":\n",
    "    update_cluster_for_unity_catalog(cluster_id, new_runtime_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c22d74f-84c2-45ca-96a8-3608c18fd50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Upgrade Databricks Cluster Runtime, Enable Unity Catalog, and Migrate Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdb0339-6e98-435a-a042-4950fa8807a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Databricks API Endpoint\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# API Endpoints\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "CLUSTER_EDIT_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/edit\"\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Fetch the current cluster details and return them as a dictionary.\"\"\"\n",
    "    response = requests.get(CLUSTER_GET_URL, headers=HEADERS, params={\"cluster_id\": cluster_id})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  \n",
    "    else:\n",
    "        print(f\"Failed to get cluster details: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def upgrade_cluster(cluster_id, new_runtime_version, single_user_name):\n",
    "    \"\"\"Upgrade the cluster runtime version and modify other configurations, including environment variables.\"\"\"\n",
    "    # Fetch current cluster details\n",
    "    cluster_details = get_cluster_details(cluster_id)\n",
    "\n",
    "    if cluster_details:\n",
    "        # Extract environment variables (spark_env_vars) before update\n",
    "        spark_env_vars = cluster_details.get(\"spark_env_vars\", {}) \n",
    "\n",
    "        # Modify the runtime version in the cluster configuration\n",
    "        cluster_details['spark_version'] = new_runtime_version\n",
    "\n",
    "        # Prepare the payload to update the cluster\n",
    "        payload = {\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"cluster_name\": cluster_details.get(\"cluster_name\", \"Default Cluster\"), \n",
    "            \"spark_version\": cluster_details[\"spark_version\"], \n",
    "            \"node_type_id\": cluster_details[\"node_type_id\"],  \n",
    "            \"driver_node_type_id\": cluster_details[\"driver_node_type_id\"],  \n",
    "            \"num_workers\": cluster_details.get(\"num_workers\"), \n",
    "            \"autoscale\" : cluster_details.get(\"autoscale\"),\n",
    "            \"autotermination_minutes\": cluster_details.get(\"autotermination_minutes\"),  \n",
    "            \"spark_conf\": cluster_details.get(\"spark_conf\", {}),  \n",
    "            \"aws_attributes\": cluster_details.get(\"aws_attributes\", {}), \n",
    "            \"custom_tags\": cluster_details.get(\"custom_tags\", {}),  \n",
    "            \"enable_elastic_disk\": cluster_details.get(\"enable_elastic_disk\", False), \n",
    "            \"init_scripts\": cluster_details.get(\"init_scripts\", []),  \n",
    "            \"single_user_name\": single_user_name,  \n",
    "            \"data_security_mode\": \"SINGLE_USER\",  \n",
    "            \"runtime_engine\": cluster_details.get(\"runtime_engine\", \"STANDARD\"),  \n",
    "            \"spark_env_vars\": spark_env_vars, \n",
    "            \"unity_catalog_enabled\": True  \n",
    "        }\n",
    "\n",
    "        # Send the update request to Databricks API\n",
    "        response = requests.post(CLUSTER_EDIT_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "        # Log the response to see if there are any issues\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Cluster updated successfully with runtime version: {new_runtime_version}\")\n",
    "            print(f\"Environment variables before update: {spark_env_vars}\")\n",
    "            return True\n",
    "        else:\n",
    "            # Print full response to help debug the issue\n",
    "            print(f\"Failed to update cluster: {response.text}\")\n",
    "            print(f\"Response code: {response.status_code}\")\n",
    "            print(f\"Response content: {response.content}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Unable to fetch cluster details for update.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if upgrade_cluster(cluster_id, new_runtime_version, single_user_name):\n",
    "        print(\"Cluster upgraded and environment variables updated successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to upgrade the cluster.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e55401a-6949-476c-ab18-7f2ab0b87714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Update the multiple clusters at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960630cf-4575-4209-9d57-cb8c3113d494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Databricks API Endpoint\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# API Endpoints\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "CLUSTER_EDIT_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/edit\"\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Fetch the current cluster details and return them as a dictionary.\"\"\"\n",
    "    response = requests.get(CLUSTER_GET_URL, headers=HEADERS, params={\"cluster_id\": cluster_id})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  \n",
    "    else:\n",
    "        print(f\"Failed to get cluster details: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def upgrade_cluster(cluster_id, new_runtime_version, single_user_name):\n",
    "    \"\"\"Upgrade the cluster runtime version and modify other configurations, including environment variables.\"\"\"\n",
    "    # Fetch current cluster details\n",
    "    cluster_details = get_cluster_details(cluster_id)\n",
    "\n",
    "    if cluster_details:\n",
    "        # Extract environment variables (spark_env_vars) before update\n",
    "        spark_env_vars = cluster_details.get(\"spark_env_vars\", {})  \n",
    "\n",
    "        # Modify the runtime version in the cluster configuration\n",
    "        cluster_details['spark_version'] = new_runtime_version\n",
    "\n",
    "        # Prepare the payload to update the cluster\n",
    "        payload = {\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"cluster_name\": cluster_details.get(\"cluster_name\", \"Default Cluster\"),  \n",
    "            \"spark_version\": cluster_details[\"spark_version\"],  \n",
    "            \"node_type_id\": cluster_details[\"node_type_id\"], \n",
    "            \"driver_node_type_id\": cluster_details[\"driver_node_type_id\"],  \n",
    "            \"num_workers\": cluster_details.get(\"num_workers\"),  \n",
    "            \"autoscale\" : cluster_details.get(\"autoscale\"),\n",
    "            \"autotermination_minutes\": cluster_details.get(\"autotermination_minutes\"),  \n",
    "            \"spark_conf\": cluster_details.get(\"spark_conf\", {}),  \n",
    "            \"aws_attributes\": cluster_details.get(\"aws_attributes\", {}),  \n",
    "            \"custom_tags\": cluster_details.get(\"custom_tags\", {}),  \n",
    "            \"enable_elastic_disk\": cluster_details.get(\"enable_elastic_disk\", False),  \n",
    "            \"init_scripts\": cluster_details.get(\"init_scripts\", []),  \n",
    "            \"single_user_name\": single_user_name,  \n",
    "            \"data_security_mode\": \"SINGLE_USER\",  \n",
    "            \"runtime_engine\": cluster_details.get(\"runtime_engine\", \"STANDARD\"),  \n",
    "            \"spark_env_vars\": spark_env_vars,  \n",
    "            \"unity_catalog_enabled\": True  \n",
    "        }\n",
    "\n",
    "        # Send the update request to Databricks API\n",
    "        response = requests.post(CLUSTER_EDIT_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "        # Log the response to see if there are any issues\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Cluster {cluster_id} updated successfully with runtime version: {new_runtime_version}\")\n",
    "            print(f\"Environment variables before update: {spark_env_vars}\")\n",
    "            return True\n",
    "        else:\n",
    "            # Print full response to help debug the issue\n",
    "            print(f\"Failed to update cluster {cluster_id}: {response.text}\")\n",
    "            print(f\"Response code: {response.status_code}\")\n",
    "            print(f\"Response content: {response.content}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"Unable to fetch cluster details for {cluster_id} for update.\")\n",
    "        return False\n",
    "\n",
    "def upgrade_multiple_clusters(cluster_ids, new_runtime_version, single_user_name):\n",
    "    \"\"\"Upgrade multiple clusters by iterating through a list of cluster IDs.\"\"\"\n",
    "    for cluster_id in cluster_ids:\n",
    "        print(f\"Upgrading cluster: {cluster_id}\")\n",
    "        if upgrade_cluster(cluster_id, new_runtime_version, single_user_name):\n",
    "            print(f\"Cluster {cluster_id} upgraded successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to upgrade cluster {cluster_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upgrade_multiple_clusters(cluster_ids, new_runtime_version, single_user_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4e2b2e-73c9-4571-8b60-f0430b02f969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Creating an External Volume in Databricks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aafd4c84-f714-4485-8a5b-c6b3d6cb1ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE EXTERNAL VOLUME my_volume\n",
    "LOCATION 's3://aws-glue11/my_volume_path/';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d3da5c-deb2-471f-b5b9-1ef059d1728e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Displaying Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588e2741-e9ad-47aa-994e-2c3e855d5892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show volumes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5ece44-21fa-4ab0-86de-c26255865962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Python Script for Upgrading a Databricks Cluster and Adding Environment Variable \"my_volume_path\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372ffadb-04c8-44f6-8d7e-640a79b50a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# Databricks API Endpoint\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "# API Endpoints\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "CLUSTER_EDIT_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/edit\"\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Fetch the current cluster details and return them as a dictionary.\"\"\"\n",
    "    response = requests.get(CLUSTER_GET_URL, headers=HEADERS, params={\"cluster_id\": cluster_id})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  \n",
    "    else:\n",
    "        print(f\"Failed to get cluster details: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def upgrade_cluster(cluster_id, new_runtime_version, single_user_name, my_volume_path):\n",
    "    \"\"\"Upgrade the cluster runtime version and modify other configurations, including environment variables.\"\"\"\n",
    "    # Fetch current cluster details\n",
    "    cluster_details = get_cluster_details(cluster_id)\n",
    "\n",
    "    if cluster_details:\n",
    "        # Extract environment variables (spark_env_vars) before update\n",
    "        spark_env_vars = cluster_details.get(\"spark_env_vars\", {})  \n",
    "\n",
    "        # Add the new environment variable for volume path\n",
    "        spark_env_vars[\"MY_VOLUME_PATH\"] = my_volume_path  \n",
    "\n",
    "        # Modify the runtime version in the cluster configuration\n",
    "        cluster_details['spark_version'] = new_runtime_version\n",
    "\n",
    "        # Prepare the payload to update the cluster\n",
    "        payload = {\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"cluster_name\": cluster_details.get(\"cluster_name\", \"Default Cluster\"),  \n",
    "            \"spark_version\": cluster_details[\"spark_version\"],  \n",
    "            \"node_type_id\": cluster_details[\"node_type_id\"],  \n",
    "            \"driver_node_type_id\": cluster_details[\"driver_node_type_id\"],  \n",
    "            \"num_workers\": cluster_details.get(\"num_workers\"),\n",
    "            \"autoscale\" : cluster_details.get(\"autoscale\"),  \n",
    "            \"autotermination_minutes\": cluster_details.get(\"autotermination_minutes\"), \n",
    "            \"spark_conf\": cluster_details.get(\"spark_conf\", {}),  \n",
    "            \"aws_attributes\": cluster_details.get(\"aws_attributes\", {}), \n",
    "            \"custom_tags\": cluster_details.get(\"custom_tags\", {}),  \n",
    "            \"enable_elastic_disk\": cluster_details.get(\"enable_elastic_disk\", False),  \n",
    "            \"init_scripts\": cluster_details.get(\"init_scripts\", []),  \n",
    "            \"single_user_name\": single_user_name,  \n",
    "            \"data_security_mode\": \"SINGLE_USER\",  \n",
    "            \"runtime_engine\": cluster_details.get(\"runtime_engine\", \"STANDARD\"),  \n",
    "            \"spark_env_vars\": spark_env_vars,  \n",
    "            \"unity_catalog_enabled\": True  \n",
    "        }\n",
    "\n",
    "        # Send the update request to Databricks API\n",
    "        response = requests.post(CLUSTER_EDIT_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "        # Log the response to see if there are any issues\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Cluster updated successfully with runtime version: {new_runtime_version}\")\n",
    "            print(f\"Environment variables before update: {spark_env_vars}\")\n",
    "            return True\n",
    "        else:\n",
    "            # Print full response to help debug the issue\n",
    "            print(f\"Failed to update cluster: {response.text}\")\n",
    "            print(f\"Response code: {response.status_code}\")\n",
    "            print(f\"Response content: {response.content}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Unable to fetch cluster details for update.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     upgrade_cluster(cluster_id, new_runtime_version, single_user_name, my_volume_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a3b520-786f-46de-8f55-7319104f58ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Reconciliation for Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b18275a2-cc47-42f2-9704-60cfc9906081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Databricks API Endpoint\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def load_cluster_backup(cluster_id):\n",
    "    \"\"\"Load the backup file for the cluster from the mounted volume.\"\"\"\n",
    "    backup_path = f\"{my_volume_path}/cluster_backup_{cluster_id}.json\"\n",
    "    if os.path.exists(backup_path):\n",
    "        with open(backup_path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        print(f\"No backup found for cluster {cluster_id}\")\n",
    "        return None\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Fetch the current cluster details and return them as a dictionary.\"\"\"\n",
    "    response = requests.get(CLUSTER_GET_URL, headers=HEADERS, params={\"cluster_id\": cluster_id})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to get cluster details: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def reconcile_cluster_details(current_details, backup_details):\n",
    "    \"\"\"Compare current cluster details with backup (focusing on key fields).\"\"\"\n",
    "    # Define the keys we want to compare\n",
    "    keys_to_compare = [\n",
    "        'cluster_name', \n",
    "        'spark_env_vars', \n",
    "        'autotermination_minutes', \n",
    "        'photon', \n",
    "        'spark_version'\n",
    "    ]\n",
    "    \n",
    "    discrepancies = []\n",
    "\n",
    "    # Compare the selected keys between current and backup details\n",
    "    for key in keys_to_compare:\n",
    "        current_value = current_details.get(key)\n",
    "        backup_value = backup_details.get(key)\n",
    "        \n",
    "        if current_value != backup_value:\n",
    "            discrepancies.append({\n",
    "                \"key\": key,\n",
    "                \"current_value\": current_value,\n",
    "                \"backup_value\": backup_value\n",
    "            })\n",
    "\n",
    "    return discrepancies\n",
    "\n",
    "def print_reconciliation_output(cluster_id, current_details, discrepancies):\n",
    "    \"\"\"Print a detailed and user-friendly reconciliation report.\"\"\"\n",
    "    print(f\"\\n--- Reconciliation Report for Cluster {cluster_id} ---\\n\")\n",
    "\n",
    "    if discrepancies:\n",
    "        print(\"Discrepancies found between current cluster and backup:\\n\")\n",
    "        for discrepancy in discrepancies:\n",
    "            print(f\"Field: {discrepancy['key']}\")\n",
    "            print(f\"  Current Value: {discrepancy['current_value']}\")\n",
    "            print(f\"  Backup Value: {discrepancy['backup_value']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No discrepancies found.\")\n",
    "\n",
    "def check_unity_catalog(current_details, backup_details):\n",
    "    \"\"\"Check Unity Catalog status in both current and backup details.\"\"\"\n",
    "    current_data_security_mode = current_details.get('data_security_mode', 'Not Available')\n",
    "    backup_data_security_mode = backup_details.get('data_security_mode', 'Not Available')\n",
    "\n",
    "    print(\"\\n--- Unity Catalog Check ---\")\n",
    "    if current_data_security_mode != backup_data_security_mode:\n",
    "        print(f\"Discrepancy in Unity Catalog:\")\n",
    "        print(f\"  Current Data Security Mode: {current_data_security_mode}\")\n",
    "        print(f\"  Backup Data Security Mode: {backup_data_security_mode}\")\n",
    "    else:\n",
    "        print(f\"Unity Catalog status is the same in both current and backup (Data Security Mode: {current_data_security_mode})\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Cluster Summary ---\")\n",
    "    # Summary of the cluster update\n",
    "    current_runtime_version = current_details.get('spark_version', 'Not Available')\n",
    "    environment_variables_before = current_details.get('spark_env_vars', {})\n",
    "    \n",
    "    print(f\"\\nCluster updated successfully with runtime version: {current_runtime_version}\")\n",
    "    print(f\"Environment variables before update: {json.dumps(environment_variables_before, indent=4)}\")\n",
    "    print(f\"Autotermination minutes: {current_details.get('autotermination_minutes', 'Not Available')}\")\n",
    "    print(f\"Photon: {current_details.get('photon', 'Not Available')}\")\n",
    "\n",
    "    print(\"\\nCluster reconciliation completed successfully.\\n\")\n",
    "\n",
    "\n",
    "def perform_data_reconciliation(cluster_id):\n",
    "    \"\"\"Perform reconciliation for a single cluster.\"\"\"\n",
    "    # Load the current cluster details\n",
    "    current_cluster_details = get_cluster_details(cluster_id)\n",
    "    \n",
    "    if current_cluster_details:\n",
    "        # Load the backup of the cluster details\n",
    "        backup_cluster_details = load_cluster_backup(cluster_id)\n",
    "        \n",
    "        if backup_cluster_details:\n",
    "            # Perform reconciliation between the current and backup details\n",
    "            discrepancies = reconcile_cluster_details(current_cluster_details, backup_cluster_details)\n",
    "            # Print reconciliation report, passing current cluster details to the summary function\n",
    "            print_reconciliation_output(cluster_id, current_cluster_details, discrepancies)\n",
    "        else:\n",
    "            print(f\"Skipping reconciliation for cluster {cluster_id} due to missing backup.\")\n",
    "    else:\n",
    "        print(f\"Skipping reconciliation for cluster {cluster_id} due to missing current details.\")\n",
    "\n",
    "\n",
    "# Example: Perform reconciliation for a specific cluster\n",
    "# cluster_id = \"your-cluster-id-here\" \n",
    "perform_data_reconciliation(cluster_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68bb7a13-e7dc-43a0-9607-b2ca5e293522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Backup your cluster with help below mentioned code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2295a2e9-8734-49ab-9622-e837eed93aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This script is designed to revert Databricks clusters to their previous state based on a backup configuration. The script fetches the current cluster details and compares them with a backup configuration (stored in a JSON file), then reverts the cluster to its previous state by updating its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf68b970-bea7-47ad-835c-d8514e923aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Databricks API Endpoint\n",
    "CLUSTER_EDIT_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/edit\"\n",
    "CLUSTER_GET_URL = f\"{DATABRICKS_INSTANCE}/api/2.0/clusters/get\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def load_cluster_details_from_file(cluster_id):\n",
    "    \"\"\"Load the saved cluster details from the JSON file.\"\"\"\n",
    "    file_name = f\"cluster_backup_{cluster_id}.json\" # if file in volume replace with volume path\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        print(f\"Backup file for cluster {cluster_id} not found.\")\n",
    "        return None\n",
    "\n",
    "def get_cluster_details(cluster_id):\n",
    "    \"\"\"Retrieve the current details of a cluster.\"\"\"\n",
    "    response = requests.get(f\"{CLUSTER_GET_URL}?cluster_id={cluster_id}\", headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to retrieve details for cluster {cluster_id}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def revert_cluster_to_previous_state(cluster_id):\n",
    "    \"\"\"Revert the cluster to its previous state using the backup configuration.\"\"\"\n",
    "    # Get the current cluster details\n",
    "    cluster_details = get_cluster_details(cluster_id)\n",
    "    \n",
    "    if cluster_details:\n",
    "        # Check if Unity Catalog is enabled, skip if it's already enabled\n",
    "        if cluster_details.get(\"unity_catalog_enabled\", False):\n",
    "            print(f\"Cluster {cluster_id} already has Unity Catalog enabled. Skipping update.\")\n",
    "            return False  # Skip the update if Unity Catalog is enabled\n",
    "        \n",
    "        # Load the backup configuration from the file\n",
    "        backup_details = load_cluster_details_from_file(cluster_id)\n",
    "        \n",
    "        if backup_details:\n",
    "            # Prepare the payload to update the cluster to its original state\n",
    "            payload = {\n",
    "                \"cluster_id\": cluster_id,\n",
    "                \"cluster_name\": backup_details.get(\"cluster_name\", \"Default Cluster\"),\n",
    "                \"spark_version\": backup_details.get(\"spark_version\", \"\"),\n",
    "                \"node_type_id\": backup_details.get(\"node_type_id\", \"\"),\n",
    "                \"driver_node_type_id\": backup_details.get(\"driver_node_type_id\", \"\"),\n",
    "                \"num_workers\": backup_details.get(\"num_workers\"),\n",
    "                \"autoscale\": backup_details.get(\"autoscale\"),\n",
    "                \"autotermination_minutes\": backup_details.get(\"autotermination_minutes\"),\n",
    "                \"spark_conf\": backup_details.get(\"spark_conf\", {}),\n",
    "                \"aws_attributes\": backup_details.get(\"aws_attributes\", {}),\n",
    "                \"custom_tags\": backup_details.get(\"custom_tags\", {}),\n",
    "                \"enable_elastic_disk\": backup_details.get(\"enable_elastic_disk\", False),\n",
    "                \"init_scripts\": backup_details.get(\"init_scripts\", []),\n",
    "                \"single_user_name\": backup_details.get(\"single_user_name\", \"\"),\n",
    "                \"data_security_mode\": backup_details.get(\"data_security_mode\", \"SINGLE_USER\"),\n",
    "                \"runtime_engine\": backup_details.get(\"runtime_engine\", \"STANDARD\"),\n",
    "                \"spark_env_vars\": backup_details.get(\"spark_env_vars\", {}),\n",
    "                \"unity_catalog_enabled\": backup_details.get(\"unity_catalog_enabled\", False)  \n",
    "            }\n",
    "\n",
    "            # Send the update request to Databricks API\n",
    "            response = requests.post(CLUSTER_EDIT_URL, headers=HEADERS, json=payload)\n",
    "\n",
    "            # Log the response to see if there are any issues\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Cluster {cluster_id} reverted to its previous state successfully.\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to revert cluster {cluster_id}: {response.text}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Unable to load backup details for cluster {cluster_id}.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"Unable to retrieve details for cluster {cluster_id}.\")\n",
    "        return False\n",
    "\n",
    "def revert_multiple_clusters(cluster_ids):\n",
    "    \"\"\"Revert multiple clusters to their previous states by iterating through a list of cluster IDs.\"\"\"\n",
    "    for cluster_id in cluster_ids:\n",
    "        print(f\"Reverting cluster: {cluster_id}\")\n",
    "        if revert_cluster_to_previous_state(cluster_id):\n",
    "            print(f\"Cluster {cluster_id} reverted successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to revert cluster {cluster_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    # Revert clusters\n",
    "    if cluster_ids_to_revert:\n",
    "        print(f\"Reverting {len(cluster_ids_to_revert)} clusters to their previous state.\")\n",
    "        revert_multiple_clusters(cluster_ids_to_revert)\n",
    "    else:\n",
    "        print(\"No cluster IDs provided for reverting.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Cluster_Migration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
