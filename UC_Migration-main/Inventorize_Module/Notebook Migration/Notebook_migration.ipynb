{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e929ff43-04c2-4769-81b0-0345ea34fdf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Databricks Notebook Path Migration and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d782a0f5-d395-45c5-9be7-c9e2c2665fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Install Pyyaml library\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e60d65-e6ca-4050-b095-1fc0313e0237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import the library\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2384ad90-42da-4fce-ada1-5deb6cc104b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration from the YAML file\n",
    "with open(\"/Workspace/Users/prasaddhumal2145@gmail.com/config.yaml\", 'r') as file:\n",
    "    config = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d11e53a-1dfc-4194-8aa3-5edf0331d184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extract configuration values from the loaded YAML file\n",
    "DATABRICKS_Host = config.get('DATABRICKS_Host')\n",
    "DATABRICKS_TOKEN = config.get('DATABRICKS_TOKEN')\n",
    "UC_CATALOG_NAME = config.get('UC_CATALOG_NAME')\n",
    "UPDATED_DIRECTORY = config.get('UPDATED_DIRECTORY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "685bcf16-1372-43c6-badf-a37ee01ad2f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Databricks Notebook Path Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f44e84f-6800-4b86-9809-53ee91ea5632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import base64\n",
    "\n",
    "databricks_host = DATABRICKS_Host   # Set your Databricks host URL\n",
    "\n",
    "# Set the UC catalog name to replace hive_metastore\n",
    "uc_catalog_name = UC_CATALOG_NAME\n",
    "\n",
    "def list_notebooks_in_directory(directory_path, headers):\n",
    "    \"\"\"List all notebooks in the specified directory, including subdirectories.\"\"\"\n",
    "    url = f\"{databricks_host}/api/2.0/workspace/list\"\n",
    "    response = requests.get(url, headers=headers, params={\"path\": directory_path})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to list notebooks: {response.status_code} - {response.text}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        return response.json().get('objects', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON response: {e}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        return []\n",
    "\n",
    "def read_notebook_content(notebook_path, headers):\n",
    "    \"\"\"Read the content of a notebook from Databricks.\"\"\"\n",
    "    url = f\"{databricks_host}/api/2.0/workspace/export\"\n",
    "    response = requests.get(url, headers=headers, params={\"path\": notebook_path, \"format\": \"SOURCE\"})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        content = response.json().get('content', '')\n",
    "        return base64.b64decode(content).decode('utf-8')\n",
    "    else:\n",
    "        print(f\"Failed to read notebook content for {notebook_path}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def contains_hive_metastore_path(content):\n",
    "    \"\"\"Check if notebook content contains any reference to 'hive_metastore'.\"\"\"\n",
    "    hive_path_pattern = r\"\\bhive_metastore\\.(\\w+)(\\.\\w+)?\\b\"\n",
    "    return bool(re.search(hive_path_pattern, content))\n",
    "\n",
    "def find_notebooks_with_hive_metastore(directory_path):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\"\n",
    "    }\n",
    "    notebooks_with_hive_metastore = []\n",
    "    all_notebooks = list_notebooks_in_directory(directory_path, headers)\n",
    "    \n",
    "    # Iterate over each notebook to check for hive_metastore references\n",
    "    for notebook in all_notebooks:\n",
    "        if notebook['object_type'] == 'NOTEBOOK':\n",
    "            notebook_path = notebook['path']\n",
    "            content = read_notebook_content(notebook_path, headers)\n",
    "            \n",
    "            if content and contains_hive_metastore_path(content):\n",
    "                notebooks_with_hive_metastore.append(notebook_path)\n",
    "        \n",
    "        elif notebook['object_type'] == 'DIRECTORY':\n",
    "            # Recursively check subdirectories\n",
    "            notebooks_with_hive_metastore.extend(find_notebooks_with_hive_metastore(notebook['path']))\n",
    "    \n",
    "    return notebooks_with_hive_metastore\n",
    "\n",
    "def update_notebook_paths(content):\n",
    "    # Pattern to match only full `hive_metastore.database.table` references, ignoring comments and standalone words\n",
    "    hive_path_pattern = r\"\\bhive_metastore\\.(\\w+)(\\.\\w+)\\b\"\n",
    "    \n",
    "    # Replace with the UC catalog, preserving database and table\n",
    "    updated_content = re.sub(hive_path_pattern, f\"{uc_catalog_name}.\\\\1\\\\2\", content)\n",
    "    return updated_content\n",
    "\n",
    "def update_notebook_in_databricks(notebook_path, updated_content, headers):\n",
    "    url = f\"{databricks_host}/api/2.0/workspace/import\"\n",
    "    encoded_content = base64.b64encode(updated_content.encode('utf-8')).decode('utf-8')\n",
    "    data = {\n",
    "        \"path\": notebook_path,\n",
    "        \"format\": \"SOURCE\",\n",
    "        \"language\": \"PYTHON\",  # Adjust this based on the notebook language\n",
    "        \"content\": encoded_content,\n",
    "        \"overwrite\": True  # Overwrite existing notebooks\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully updated {notebook_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to update notebook {notebook_path}: {response.text}\")\n",
    "\n",
    "def update_notebooks_with_hive_metastore(directory_path):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\"\n",
    "    }\n",
    "    \n",
    "    # Find all notebooks that need migration\n",
    "    notebooks_to_update = find_notebooks_with_hive_metastore(directory_path)\n",
    "    \n",
    "    # Show the list of notebooks needing migration\n",
    "    if notebooks_to_update:\n",
    "        print(\"Notebooks needing migration:\")\n",
    "        for notebook_path in notebooks_to_update:\n",
    "            print(notebook_path)\n",
    "    else:\n",
    "        print(\"No notebooks with 'hive_metastore' references found.\")\n",
    "\n",
    "    # Update each notebook with hive_metastore paths\n",
    "    for notebook_path in notebooks_to_update:\n",
    "        content = read_notebook_content(notebook_path, headers)\n",
    "        if content:\n",
    "            updated_content = update_notebook_paths(content)\n",
    "            update_notebook_in_databricks(notebook_path, updated_content, headers)\n",
    "\n",
    "# Run the function to find and update notebooks with hive_metastore references\n",
    "update_notebooks_with_hive_metastore(UPDATED_DIRECTORY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48aa3517-2554-472b-ac34-33846b96a541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Testing script that checks if the notebooks have the correct path after the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc20c79-9615-483a-ba64-39b548f025b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import re\n",
    "\n",
    "# Databricks host and token\n",
    "databricks_host = DATABRICKS_Host\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\" # Replace with your token\n",
    "}\n",
    "\n",
    "# UC catalog name\n",
    "uc_catalog_name = UC_CATALOG_NAME\n",
    "\n",
    "# Path to check for changes\n",
    "directory_path = UPDATED_DIRECTORY\n",
    "\n",
    "def check_notebook_paths_in_directory(directory_path):\n",
    "    # List all notebooks in the directory\n",
    "    url = f\"{databricks_host}/api/2.0/workspace/list\"\n",
    "    response = requests.get(url, headers=headers, params={\"path\": directory_path})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to list notebooks: {response.status_code} - {response.text}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        notebooks = response.json().get('objects', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse JSON response: {e}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        return\n",
    "\n",
    "    # Iterate over notebooks and check paths\n",
    "    for notebook in notebooks:\n",
    "        if notebook['object_type'] == 'NOTEBOOK':\n",
    "            notebook_path = notebook['path']\n",
    "            print(f\"Checking notebook: {notebook_path}\")\n",
    "            \n",
    "            # Read notebook content\n",
    "            content = read_notebook_content(notebook_path)\n",
    "            if content:\n",
    "                # Check if Hive paths have been replaced\n",
    "                check_path_replacement(content, notebook_path)\n",
    "\n",
    "        elif notebook['object_type'] == 'DIRECTORY':\n",
    "            # Recursively check notebooks in directories\n",
    "            check_notebook_paths_in_directory(notebook['path'])\n",
    "\n",
    "def read_notebook_content(notebook_path):\n",
    "    url = f\"{databricks_host}/api/2.0/workspace/export\"\n",
    "    response = requests.get(url, headers=headers, params={\"path\": notebook_path, \"format\": \"SOURCE\"})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        content = response.json().get('content', '')\n",
    "        return base64.b64decode(content).decode('utf-8')\n",
    "    else:\n",
    "        print(f\"Failed to read notebook content for {notebook_path}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "def check_path_replacement(content, notebook_path):\n",
    "    # Check for any remaining 'hive_metastore' references\n",
    "    if \"hive_metastore\" in content:\n",
    "        print(f\"Path not updated correctly in: {notebook_path}\")\n",
    "    else:\n",
    "        # Check if it now contains references to the UC catalog\n",
    "        uc_pattern = f\"{uc_catalog_name}\\\\.\"\n",
    "        if re.search(uc_pattern, content):\n",
    "            print(f\"Notebook paths updated correctly in: {notebook_path}\")\n",
    "        else:\n",
    "            print(f\"No Hive or UC references found in: {notebook_path}\")\n",
    "\n",
    "# Run the check process on your notebook directory\n",
    "check_notebook_paths_in_directory(UPDATED_DIRECTORY)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Notebook_migration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
